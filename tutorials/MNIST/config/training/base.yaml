training:
    fp16: True
    num_gpus_per_node: 1
    batch_size: 32
    gradient_accumulation_batches: 1
    resume:
        resume: False
    total_num:
        epochs: 20
        update_steps: -1 # disabled when total_num.epochs < 0