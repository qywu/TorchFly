training:
    fp16: True
    num_gpus_per_node: 1
    batch_size: 32
    gradient_accumulation_batches: 1
    resume:
        enabled: False
    evaluation:
        save_top_k_models: 3
    total_num:
        epochs: 5
        update_steps: -1 # disabled when total_num.epochs < 0
