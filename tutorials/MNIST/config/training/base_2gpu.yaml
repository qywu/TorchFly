training:
    fp16: True
    num_gpus_per_node: 2
    batch_size: 32
    gradient_accumulation_batches: 1
    resume:
        enabled: False
        resume_model: True
        resume_optimizer: True
        resume_scheduler: True
        resume_rng_state: True
    checkpointing:
        enabled: True
        async_save: False
        directory: "Checkpoints"
        steps_interval: 1000
        seconds_interval: -1
        num_checkpoints_to_keep: 3
        keep_checkpoint_every_num_seconds: 86400 # every 24 hrs
    evaluation:
        save_top_k_models: 3
    total_num:
        epochs: 5
        update_steps: -1 # disabled when total_num.epochs < 0
